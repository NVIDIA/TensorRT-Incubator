# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import base64
import inspect
import weakref
from typing import Dict, Sequence, Tuple, Union

import mlir_tensorrt.runtime.api as runtime
import tensorrt as trt
from nvtripy import config, export
from nvtripy.backend.api.input_info import InputInfo
from nvtripy.backend.api.stream import default_stream
from nvtripy.backend.mlir import utils as mlir_utils
from nvtripy.backend.mlir.memref import create_memref
from nvtripy.backend.mlir.utils import MLIRRuntimeClient
from nvtripy.common import datatype
from nvtripy.common.exception import raise_error
from nvtripy.frontend import Tensor
from nvtripy.trace.ops.constant import Constant
from nvtripy.utils import json as json_utils
from nvtripy.utils.types import str_from_type_annotation

import torch


USE_TRT_RUNTIME = True

TP_FROM_TRT_DTYPE = {
    trt.float32: datatype.float32,
    trt.float16: datatype.float16,
    trt.bfloat16: datatype.bfloat16,
    trt.int8: datatype.int8,
    trt.int32: datatype.int32,
    trt.int64: datatype.int64,
    trt.bool: datatype.bool,
    trt.fp8: datatype.float8,
    trt.int4: datatype.int4,
}


class OutputAllocator(trt.IOutputAllocator):
    def __init__(self, stream):
        trt.IOutputAllocator.__init__(self)
        self.memrefs = {}
        self.shapes = {}
        self.stream = stream

    def reallocate_output(self, tensor_name, memory, size, alignment):
        shape = (size,)
        # TODO: Add reallocation if size changes:
        if tensor_name not in self.memrefs:
            # TODO (pranavm): Need custom allocation as memrefs are tied to client and will be freed too early.
            self.memrefs[tensor_name] = torch.empty(shape, dtype=torch.int8, device="cuda")
            # self.memrefs[tensor_name] = create_memref(shape, dtype=datatype.int8, stream=self.stream)
        return self.memrefs[tensor_name].data_ptr()

    def notify_shape(self, tensor_name, shape):
        self.shapes[tensor_name] = tuple(shape)


# Executable.__call__ is in the hot path for benchmarks, so we would not want additional overhead
@export.public_api(document_under="compiling_code", bypass_dispatch=["__call__"], document_init_sig=False)
class Executable:
    """
    Represents a compiled executable generated by the compiler.

    .. seealso:: :func:`compile`
    """

    # The constructor is intentionally undocumented because it is not meant to be called by users.
    # `return_single_tensor_as_sequence` indicates whether the return type should be a sequence even if
    # there is only one output.
    def __init__(
        self, executable, arg_names, return_single_tensor_as_sequence: bool, input_infos: Dict[str, InputInfo]
    ):
        self._executable = executable

        self._runtime_client = MLIRRuntimeClient()
        # TODO (#577): Support multiple devices:
        self._session = runtime.RuntimeSession(runtime.RuntimeSessionOptions(num_devices=1, device_id=0), executable)
        # TODO (pranavm): Make stream a property so we can update the output allocator stream.
        self.stream = default_stream()

        self._arg_names = arg_names
        self._num_expected_args = len(arg_names)
        self._executable_signature = self._executable.get_signature("main")
        self._return_single_tensor_as_sequence = return_single_tensor_as_sequence

        # Build a signature so the executable works with `inspect.signature`
        params = []
        for name in self._arg_names:
            params.append(inspect.Parameter(name, inspect.Parameter.POSITIONAL_OR_KEYWORD, annotation=Tensor))

        num_outputs = self._executable_signature.get_num_results()

        return_annotation = (
            Tuple[(Tensor,) * num_outputs] if num_outputs > 1 or self._return_single_tensor_as_sequence else Tensor
        )

        self.__signature__ = inspect.Signature(params, return_annotation=return_annotation)

        self.input_infos: Dict[str, InputInfo] = input_infos
        """
        Stores metadata, like shapes and data types, for each input to the executable.
        """

        if USE_TRT_RUNTIME:
            self.runtime = trt.Runtime(trt.Logger())
            self.engine = self.runtime.deserialize_cuda_engine(self.serialized_tensorrt_engine)
            self.context = self.engine.create_execution_context()
            self.output_allocator = OutputAllocator(self.stream)

        # from polygraphy.backend.trt import TrtRunner, EngineFromBytes

        # self.runner = TrtRunner(EngineFromBytes(self.serialized_tensorrt_engine))
        # self.runner.activate()
        # self.runner.output_allocator.set_use_torch(True)
        # self.runner.output_allocator.set_use_torch = lambda use_torch: None
        # self._finalizer = weakref.finalize(self, lambda runner: runner.deactivate(), self.runner)

    def __str__(self) -> str:
        params = [
            f"{name}: {str_from_type_annotation(param.annotation)}"
            for name, param in self.__signature__.parameters.items()
        ]
        return f"Executable({', '.join(params)}) -> {str_from_type_annotation(self.__signature__.return_annotation)}"

    @staticmethod
    def load(path: str) -> "nvtripy.Executable":
        """
        Loads a executable from the provided path.

        Args:
            path: The path from which to load the exectuable.

        Returns:
            The executable object loaded from the file.

        .. code-block:: python
            :linenos:
            :caption: Save and load executable

            import os
            import tempfile # doc: omit

            def add(a, b):
                return a + b

            # doc: no-print-locals compiled_add executable_file
            compiled_add = tp.compile(
                add,
                args=[
                    tp.InputInfo(shape=((1, 2, 3),), dtype=tp.float32),
                    tp.InputInfo(shape=((1, 2, 3),), dtype=tp.float32),
                ],
            )


            out_dir = tempfile.TemporaryDirectory().name # doc: omit
            # Assuming `out_dir` is the directory containing the executable:
            executable_file = os.path.join(out_dir, "executable.json")
            compiled_add.save(executable_file) # doc: omit
            assert os.path.exists(executable_file)
            loaded_executable = tp.Executable.load(executable_file)
        """

        return json_utils.load(path)

    def __call__(self, *args: Tensor, **kwargs: Tensor) -> Union[Tensor, Sequence[Tensor]]:
        """
        Invokes the executable with the specified tensor arguments.

        .. note:: Inputs must be evaluated tensors in GPU memory.

            You can use :func:`nvtripy.copy` or :func:`nvtripy.Tensor.eval` to ensure this.

        Args:
            *args: Positional arguments. Must be of type :class:`Tensor` .
            **kwargs: Keyword arguments. Must be of type :class:`Tensor` .

        Returns:
            The output :class:`Tensor` s of the compiled function.


        .. code-block:: python
            :linenos:

            def add(a, b):
                return a + b

            # doc: no-print-locals compiled_add
            compiled_add = tp.compile(
                add,
                args=[
                    tp.InputInfo((1,), dtype=tp.float32),
                    tp.InputInfo((1,), dtype=tp.float32),
                ],
            )

            a = tp.ones((1,), dtype=tp.float32).eval()
            b = tp.ones((1,), dtype=tp.float32).eval()

            out = compiled_add(a, b)
        """
        num_positional = len(args)
        NUM_ARGS = num_positional + len(kwargs)

        input_tensors = list(args)
        # Need to get arguments in the order of self._arg_names, which may be different from kwargs ordering.
        expected_kwargs = self._arg_names[num_positional:]
        for name in expected_kwargs:
            if name not in kwargs:
                raise_error(f"Missing argument: {name}", [f"Expected the following arguments: {self._arg_names}"])

            input_tensors.append(kwargs[name])
            del kwargs[name]

        if kwargs:
            raise_error(
                f"Extra keyword arguments: {list(kwargs.keys())}",
                [
                    f"Expected the following arguments: {self._arg_names}.\n"
                    f"Note: The following arguments were already provided as positional arguments: {self._arg_names[:num_positional]}"
                ],
            )

        # We do this after kwarg checks since those will be more informative (we can explain which arguments are missing/extra).

        if NUM_ARGS != self._num_expected_args:
            raise_error(
                "Incorrect number of arguments.",
                [
                    f"Expected {self._num_expected_args} arguments but got {NUM_ARGS}.\n"
                    f"Note: Expected arguments were: {self._arg_names}",
                ],
            )

        for tensor in input_tensors:
            producer = tensor.trace_tensor.producer
            if not isinstance(producer, Constant) or tensor.device.kind != "gpu":
                raise_error(
                    "Inputs to compiled executables must be evaluated tensors on the GPU.",
                    [
                        "Got input" + (f" on device '{tensor.device}':" if tensor.device.kind != "gpu" else ":"),
                        tensor,
                        "Hint: Try calling `.eval()` on the tensor to ensure it is a GPU constant.",
                    ],
                )

        if USE_TRT_RUNTIME:

            def get_engine_io_names(mode):
                names = []
                for idx in range(self.engine.num_io_tensors):
                    name = self.engine.get_tensor_name(idx)
                    if self.engine.get_tensor_mode(name) == mode:
                        names.append(name)
                return names

            # The order of inputs to the engine should be the same as the order of arguments to the executable.
            engine_input_names = get_engine_io_names(trt.TensorIOMode.INPUT)
            engine_output_names = get_engine_io_names(trt.TensorIOMode.OUTPUT)

            for inp, name in zip(input_tensors, engine_input_names):
                self.context.set_input_shape(name, inp.trace_tensor.shape)
                self.context.set_tensor_address(name, inp.trace_tensor.producer.data.ptr)

            for name in engine_output_names:
                self.context.set_output_allocator(name, self.output_allocator)

            self.context.execute_async_v3(self.stream._active_cuda_stream.ptr)

            outputs = []
            for name in engine_output_names:
                output = self.output_allocator.memrefs[name]
                dtype = self.engine.get_tensor_dtype(name)

                output = self._runtime_client.create_device_memref_view(
                    output.data_ptr(),
                    self.output_allocator.shapes[name],
                    mlir_utils.convert_tripy_dtype_to_runtime_dtype(TP_FROM_TRT_DTYPE[dtype]),
                    # TODO (#577): Choose output device based on input device for multi-device support:
                    self._runtime_client.get_devices()[0],
                )
                tensor_out = Tensor.fast_init(output)
                tensor_out._mem = self.output_allocator.memrefs[name]
                outputs.append(tensor_out)

            if len(outputs) > 4:
                outputs[2], outputs[3] = outputs[3], outputs[2]
            outputs = tuple(outputs)

            if self.__signature__.return_annotation == Tensor:
                outputs = outputs[0]
            return outputs

        assert False
        # import torch

        # feed_dict = dict(
        #     zip([f"arg{i}" for i in range(len(input_tensors))], [torch.from_dlpack(inp) for inp in input_tensors])
        # )

        # # print(self.runner.get_input_metadata())
        # # print({k: v.shape for k, v in feed_dict.items()})

        # outputs = self.runner.infer(feed_dict, check_inputs=False, copy_outputs_to_host=False)

        # outputs = [Tensor.fast_init(out.contiguous()) for out in outputs.values()]

        # if len(outputs) > 4:
        #     outputs[2], outputs[3] = outputs[3], outputs[2]
        # outputs = tuple(outputs)

        # if self.__signature__.return_annotation == Tensor:
        #     return outputs[0]
        # return outputs

        input_memrefs = [inp.trace_tensor.producer.data for inp in input_tensors]
        try:
            output_memrefs = self._session.execute_function(
                "main", in_args=input_memrefs, stream=self.stream._active_cuda_stream, client=self._runtime_client
            )
        except runtime.MTRTException as err:
            # TODO: Evaluate whether this should be moved into the executor
            if "function expects a memref type with element type" in str(err):
                # If the problem is a mismatched data type, we can provide a better error message than the executor can.
                expected_input_dtypes = [info.dtype for info in self.input_infos.values()]
                for tensor, dtype, arg_name in zip(input_tensors, expected_input_dtypes, self._arg_names):
                    if tensor.dtype != dtype:
                        raise_error(
                            f"Unexpected tensor data type.",
                            (
                                [
                                    f"For parameter {arg_name}, expected data type: {dtype} but got: {tensor.dtype}. ",
                                ]
                                + (["Note: Argument was: ", tensor] if "all" in config.extra_error_information else [])
                            ),
                        )
            elif "InternalError: failed to set input shape" in str(err) or "Runtime shape mismatch" in str(err):
                expected_input_shapes = [info.shape_bounds for info in self.input_infos.values()]
                for tensor, expected_bounds, arg_name in zip(input_tensors, expected_input_shapes, self._arg_names):
                    shape = tensor.shape

                    if len(shape) != len(expected_bounds.min):
                        raise_error(
                            f"Unexpected tensor rank.",
                            [
                                f"For tensor: `{arg_name}`, expected a rank of: {len(expected_bounds.min)} but got: {len(shape)}.\n"
                                f"Note: The provided argument was: ",
                                tensor,
                            ],
                        )

                    for i in range(len(shape)):
                        if shape[i] < expected_bounds.min[i] or shape[i] > expected_bounds.max[i]:
                            raise_error(
                                f"Unexpected tensor shape.",
                                [
                                    f"For tensor: `{arg_name}`, expected a shape within the bounds: min={expected_bounds.min}, max={expected_bounds.max}, but got: {shape}.\n"
                                    f"Dimension {i} has a shape of {shape[i]}, which is not within the expected bounds of [{expected_bounds.min[i]}, {expected_bounds.max[i]}].\n"
                                    f"Note: The provided argument was: ",
                                    tensor,
                                ],
                            )
            raise_error(str(err))

        output_tensors = tuple(Tensor.fast_init(output_memref) for output_memref in output_memrefs)
        if self.__signature__.return_annotation == Tensor:
            output_tensors = output_tensors[0]
        return output_tensors

    def save(self, path: str) -> None:
        """
        Saves this executable to the provided path.

        Args:
            path: The path at which to save the executable.

        .. code-block:: python
            :linenos:
            :caption: Save executable

            import os
            import tempfile # doc: omit

            def add(a, b):
                return a + b

            # doc: no-print-locals compiled_add executable_file
            compiled_add = tp.compile(
                add,
                args=[
                    tp.InputInfo(shape=((1, 2, 3),), dtype=tp.float32),
                    tp.InputInfo(shape=((1, 2, 3),), dtype=tp.float32),
                ],
            )

            out_dir = tempfile.TemporaryDirectory().name # doc: omit
            # Assuming `out_dir` is the desired output directory:
            executable_file = os.path.join(out_dir, "executable.json")
            compiled_add.save(executable_file)
            assert os.path.exists(executable_file)
        """
        json_utils.save(self, path)

    @property
    def serialized_tensorrt_engine(self) -> bytes:
        """
        The serialized TensorRT engine, as ``bytes``, from the executable.

        .. seealso:: Refer to the `TensorRT developer guide <https://docs.nvidia.com/deeplearning/tensorrt/latest/inference-library/python-api-docs.html#deserializing-a-plan>`_
            for details on how to work with serialized TensorRT engines.

        .. code-block:: python
            :linenos:
            :caption: TensorRT engine

            def add(a, b):
                return a + b

            # doc: no-print-locals compiled_add trt_engine
            compiled_add = tp.compile(
                add,
                args=[
                    tp.InputInfo(shape=((1, 2, 3),), dtype=tp.float32),
                    tp.InputInfo(shape=((1, 2, 3),), dtype=tp.float32),
                ],
            )

            trt_engine = compiled_add.serialized_tensorrt_engine
            assert isinstance(trt_engine, bytes)
        """
        data_segments = self._executable.get_data_segments()
        if len(data_segments) != 1:
            raise_error(
                "Cannot get tensorrt engine from multiple clusters.",
                [f"Found {len(data_segments)} clusters in the executable."],
            )
        trt_cluster = data_segments[0]  # tuple of (name, data)
        return trt_cluster[1]


@json_utils.Encoder.register(Executable)
def encode_executable(executable):
    return {
        "arg_names": executable._arg_names,
        "executable": base64.b64encode(executable._executable.serialize()).decode(),
        "_return_single_tensor_as_sequence": executable._return_single_tensor_as_sequence,
        "input_infos": executable.input_infos,
    }


@json_utils.Decoder.register(Executable)
def decode_executable(executable_dict):
    executable_bytes = base64.b64decode(executable_dict["executable"])
    return Executable(
        runtime.Executable(executable_bytes),
        executable_dict["arg_names"],
        return_single_tensor_as_sequence=executable_dict["_return_single_tensor_as_sequence"],
        input_infos=executable_dict["input_infos"],
    )
