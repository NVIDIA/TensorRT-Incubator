# Adding New Operators

*You may find it helpful to read the [architecture](project:./architecture.md) documentation*
    *before you start reading this guide.*

Adding new operators to Tripy typically involves making changes in the frontend as well
as in the `FlatIR`. In some cases, the frontend operator can be expressed in terms of existing
`FlatIR` operators, in which case you only need to make changes in the frontend.

Let's take a look at an example of how you might add an `Iota` operator to Tripy.
So that it doesn't clash with Tripy's actual `Iota` implementation, we'll call it
`Theta` instead.

```{contents} Table of Contents
:depth: 3
```

<!-- Use the TEST: USE_PYTEST marker since we'll be defining unit tests as part of the guide.
    With this marker, those tests can actually be run under pytest. -->

## Implementation

### `FlatIR` Operator

The `FlatIR` operator is usually the most challenging aspect of implementing operators
in Tripy. The good news is that you might not even need to do this if the low-level operators
you need already exist in the `FlatIR`. And if you do, then it'll only get easier after this!

We'll start by adding a new file under [`tripy/flat_ir/ops`](source:/tripy/flat_ir/ops/) called
`theta.py`; see the inline comments for explanations of what's happening:

```py
from dataclasses import dataclass

from mlir_tensorrt.compiler import ir
from mlir_tensorrt.compiler.dialects import stablehlo

from tripy.flat_ir.ops.base import BaseFlatIROp


# Every `FlatIR` operator is implemented as a `dataclass` so that the base
# class can automatically implement several methods by inspecting the child
# class fields at runtime. The `repr=False` is important because the default
# `__repr__` method generated by `dataclass` will be extremely verbose and
# makes interactive debugging more difficult.
@dataclass(repr=False)
class ThetaOp(BaseFlatIROp):
    dim: int

    # `to_mlir()` is the trickiest bit. As the name implies, the method is meant to lower the
    # `FlatIR` operator into MLIR. To figure out which MLIR operators to use, refer to
    # the 'MLIR Python API Guide' (linked below).
    def to_mlir(self, operands):
        out_type = self.outputs[0].to_mlir()
        theta_dim = ir.IntegerAttr.get(type=ir.IntegerType.get_signless(64), value=self.dim)
        output = stablehlo.DynamicIotaOp(result=out_type, output_shape=operands[0], iota_dimension=theta_dim)
        return [output]
```

Links:
- [MLIR Python API Guide](project:./mlir-dialect-python-apis.md)


### Exposing The Operator

One of the principles we follow when writing submodules is that other submodules should
not need to reach into the internals of a submodule to retrieve something they need.

For example, a class which needs to import `ThetaOp` does not need to know where exactly
within the `flat_ir.ops` module the `ThetaOp` lives - it should be able to just import it
from the submodule.

To make this possible, we need to import the `ThetaOp` into the `flat_ir.ops` submodule.
We can do so by adding the following line into
[`tripy/flat_ir/ops/__init__.py`](source:/tripy/flat_ir/ops/__init__.py):


```py
from tripy.flat_ir.ops.theta import ThetaOp
```





## `Trace` Operator And The Public API

Now that we have a `FlatIR` operator, we can implement a `Trace` operator that will use it
along with a public API function. Let's create a new file under
[`tripy/frontend/trace/ops`](source:/tripy/frontend/trace/ops/) called `theta.py`.

### `Trace` Operator

First, we'll implement the `Trace` operator itself:

```py
from dataclasses import dataclass
from typing import Tuple

from tripy import utils
from tripy.common import datatype, device
from tripy.common.exception import raise_error
from tripy.frontend.trace.ops.base import BaseTraceOp
import tripy.frontend.trace.ops.utils as op_utils


# Just like with `FlatIR` operators, all `Trace` operators are implemented as `dataclass`es.
# As before, we want `repr=False` here.
@dataclass(repr=False)
class Theta(BaseTraceOp):
    # Notice that we do *not* need to define a constructor and can rely on the default
    # implementation provided by `dataclass`.
    dim: int
    dtype: datatype.dtype

    # The `infer_shape_output_idxs` method should indicate which outputs of this operator represent shapes.
    # The corresponding outputs will be wrapped as `tripy.Shape` objects instead of regular `tripy.Tensor`s.
    # Our `Theta` operation should never return shapes, so we can use the corresponding preexisting policy.
    infer_shape_output_idxs = op_utils.ShapeOutputIdxPolicies.never_return_shape

    # *Optional* `infer_dtypes()` populates the data types of the
    # output `TraceTensor`s. The default implementation copies the input
    # data types if they are all the same, so you may not need to implement this.
    def infer_dtypes(self):
        self.outputs[0].dtype = self.dtype

    # *Optional* `infer_devices()` populates the devices of the
    # output `TraceTensor`s. The default implementation copies the input
    # devices if they are all the same, so you may not need to implement this either.
    def infer_devices(self):
        self.outputs[0].device = device("gpu")

    # `infer_rank()` populates the rank of the output `TraceTensor`s.
    # For most operators, the output rank will depend on the rank of `self.inputs`.
    # In our case, since `Theta` generates a tensor, there is no input tensor.
    def infer_rank(self):
        from tripy.backend.mlir.utils import ShapeContext

        # `self.inputs[0]` indicates the desired shape of the output.
        # Here, we compute the number of elements in the shape tensor, which determines the rank of the output.
        out_shape = ShapeContext().get_shape_of_dynamic_trace_tensor(self.inputs[0])
        assert len(out_shape) == 1, f"Expected rank of shape tensor to be 1, got {len(out_shape)}"
        assert (
            out_shape[0] >= 0
        ), f"Incorrect shape of shape tensor, expected shape to be positive, got {out_shape[0]}"
        self.outputs[0].rank = out_shape[0]

    # `to_flat_ir()` translates the `Trace` operator to a subgraph of
    # one or more `FlatIR` operators. In our case, it's just a 1:1
    # mapping to the `ThetaOp` we created earlier.
    def to_flat_ir(self, inputs, outputs):
        # Note that we import the `FlatIR` operator within the function
        # call - this is to avoid circular dependencies.
        from tripy.flat_ir.ops import ThetaOp
        import tripy.frontend.trace.ops.utils as op_utils

        # This code may look a bit confusing; for more details, look at the
        # 'FlatIR section in the architecture document' (linked below).
        ThetaOp.build(inputs, outputs, dim=self.dim)
```

Links:
- [FlatIR section in the architecture document](project:./architecture.md#lowering-to-flatir)


### Public API

Next, we can define the public interface. Since our public interface maps 1:1 with the `Trace`
operator we just implemented and does not require weights, we'll add it in the same file.

If our API required a composition of multiple `Trace` operators, then we would instead implement
it under [`frontend/ops/`](source:/tripy/frontend/ops).

If it required weights (i.e. inputs that are expected to always be constant), then we would implement
it as a `tripy.Module` under [`frontend/module`](source:/tripy/frontend/module).

```py
from tripy import export
import tripy.frontend.utils as frontend_utils

# We can use the `export.public_api()` decorator to automatically export this function into the
# top-level module. This means it will be accessible as `tripy.theta`.
#
# This decorator also controls how the API is exposed in the documentation - the `document_under`
# option determines where in the documentation hierarchy this API will show up.
#
# If we needed to provide any special autodoc options, we could use the `autodoc_options` parameter.
@export.public_api(document_under="tensor_operations")

# The `convert_inputs_to_tensors` decorator converts function arguments to Tensors.
# This is what makes it possible for the user to use Python numbers in Tripy functions (e.g. `tensor + 1`)
# In this case, we want `shape` to turn into a `tripy.Shape` instead of a regular `Tensor`.
@frontend_utils.convert_inputs_to_tensors(shape_argument=["shape"], exclude=["dim", "dtype"])
def theta(shape: Tuple[int], dim: int = 0, dtype: datatype.dtype = datatype.float32) -> "tripy.Tensor":
    # For any public facing interfaces, we have documentation requirements which you can read
    # about in the 'Docs README' (linked below). The docstring we've implemented here
    # adheres to all of these requirements. Non-compliant docstrings will, in most cases,
    # cause test failures; however, you should still manually ensure you're writing high-quality
    # docstrings.
    #
    # The examples in docstrings are run as part of our tests, so you should also add
    # assertions to make sure things are functionally correct. In this case, we check
    # that the `output` we create in the code example is what we expect.
    """
    Fills an output tensor with consecutive values starting from zero along the given dimension.

    Args:
        shape: The desired shape.
        dim: Dimension along which to perform the theta operation.
            This cannot exceed the rank of the specified shape.
        dtype: The desired data type.

    Returns:
        A tensor of shape ``shape`` and data type ``dtype``.

    .. code-block:: python
        :linenos:
        :caption: Example

        output = tp.theta([3])

        assert np.array_equal(cp.from_dlpack(output).get(), np.arange(0, 3, dtype=np.float32))
    """

    # Next we build the trace operator. The `build()` function is also responsible for constructing
    # the output frontend Tensors. All of the arguments that follow the inputs
    # are forwarded directly to the constructor of the `Trace` operator.
    return Theta.build([shape], dim, dtype)

```



Links:
- [Docs README](source:/docs/README.md#docstrings)


### Exposing The Operator

Similarly to the `FlatIR` operator, we need to import `Theta` into the
`frontend.trace.ops` submodule. We can do so by adding the following line into
[`tripy/frontend/trace/ops/__init__.py`](source:/tripy/frontend/trace/ops/__init__.py):


```py
from tripy.frontend.trace.ops.theta import Theta, theta
```



## Testing

Now that we've implemented our operator, let's write tests for it. The structure of the
[`tests/`](source:/tests/) directory mirrors that of the [`tripy/`](source:/tripy/) directory
(you can read more about that [here](source:/tests/README.md)). We need to test both the `FlatIR`
and `Trace` operators.

### Testing The `FlatIR` Operator

When testing our `FlatIR` operator, we essentially need to test two things:

1. Is the string reprensetation of the operator correct? We need to make sure it is since this
    is what will appear in the `FlatIR` dumps.

2. Is the translation to MLIR correct?

Since we implemented the `FlatIR` operator in [`tripy/flat_ir/ops`](source:/tripy/flat_ir/ops/), we'll
add the corresponding test under [`tests/flat_ir/ops`](source:/tests/flat_ir/ops/). Create a new file
there called `test_theta.py`.

We'll start by defining a pytest fixture that will generate a `FlatIR` containing a `ThetaOp` for us.
To do so, we can simply use the public API, generate a `Trace`, and convert to `FlatIR`:

```py
import pytest
import re

import tripy as tp
from tests import helper
from tripy.frontend.trace import Trace
from tripy.flat_ir.ops import IotaOp


@pytest.fixture
def flat_ir():
    out = tp.theta((2, 3))
    out.name = "out"

    trace = Trace([out])
    yield trace.to_flat_ir()
```

Now we can create a test class with our two tests:

```py
class TestThetaOp:
    # This tests the string representation of our `FlatIR` operator.
    # This may be hard to predict, so we suggest that you first `print(str(Theta))`,
    # check if it looks correct, and then add the corresponding string to the test.
    def test_str(self, flat_ir):
        Theta = flat_ir.ops[-1]
        assert isinstance(Theta, ThetaOp)
        assert re.match(
            r"out: \[rank=\(2\), dtype=\(float32\), loc=\(gpu:0\)\] = ThetaOp\(t[0-9]+, dim=0\)",
            str(Theta),
        )


    # This tests conversion to MLIR by checking the generated MLIR module code. Once again,
    # this is difficult to predict ahead of time, so you should print the MLIR module once,
    # check if it looks correct, and then update the test accordingly.
    def test_mlir(self, flat_ir):
        helper.check_mlir(
            flat_ir.to_mlir(),
            """
            module {
                func.func @main() -> tensor<?x?xf32> {
                    %c = stablehlo.constant dense<[2, 3]> : tensor<2xi32>
                    %0 = stablehlo.dynamic_iota %c, dim = 0 : (tensor<2xi32>) -> tensor<?x?xf32>
                    return %0 : tensor<?x?xf32>
                }
            }
            """,
        )
```


### Testing The Trace Operator And Public API

Since we implemented our `Trace` operator and public API in
[`tripy/frontend/trace/ops`](source:/tripy/frontend/trace/ops/), we'll add the test under
[`tests/frontend/trace/ops`](source:/tests/frontend/trace/ops/).
Create a new file there called `test_theta.py`:


```py
import tripy as tp
from tests import helper
from tripy.frontend.trace.ops import Theta


class TestTheta:
    # This ensures that the public API function creates a frontend `Tensor`
    # and populates it with the right `Trace` operator.
    def test_op_func(self):
        a = tp.theta([2, 3])
        assert isinstance(a, tp.Tensor)
        assert isinstance(a.trace_tensor.producer, Theta)

    # You should also include negative tests for anything that is expected to
    # fail. In our case, we just have `test_invalid_dim`,
    # which ensures that we emit an error if the `dim` parameter is outside
    # the allowed range.
    def test_invalid_dim(self):
        with helper.raises(tp.TripyException, match="iota dimension cannot go beyond the output rank or be negative."):
            tp.theta([2, 3], dim=3).eval()
```


### Integration Tests

The code examples in the docstring of the public API serve as good sanity integration tests.
However, you should still add separate integration tests to get better coverage.

Our docstring covers the 1D case, so let's add an integration test to cover the multidimensional case.
Create a new file called `test_theta.py` under [`tests/integration`](source:/tests/integration/):

```py
import numpy as np
import cupy as cp

import tripy as tp


def test_multi_dimensional():
    output = tp.theta([2, 3], dim=1)
    expected = np.broadcast_to(np.arange(0, 3, dtype=np.float32), (2, 3))

    assert np.array_equal(cp.from_dlpack(output).get(), expected)

```

## Done!

If you've reached this point, you have successfully added a new operation to
Tripy. Congratulations!
