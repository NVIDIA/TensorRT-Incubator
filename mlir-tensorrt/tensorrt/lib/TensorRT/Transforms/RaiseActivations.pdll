//===- GeluActivationPatterns.pdll ------------------------------*- PDLL -*-===//
//
// SPDX-FileCopyrightText: Copyright 2024 NVIDIA CORPORATION & AFFILIATES.
// All rights reserved.
// SPDX-License-Identifier: Apache-2.0
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
//===----------------------------------------------------------------------===//
//
// PDLL rewrites for matching GELU.
//
//===----------------------------------------------------------------------===//
#include "mlir-tensorrt-dialect/TensorRT/IR/TensorRTOps.td"


//===----------------------------------------------------------------------===//
// Constraint definitions for primitives (mul, add, etc).
// Each constraint requires 3 `Constraint` functions because of PDLL
// limitations involving concrete enum attributes...
//===----------------------------------------------------------------------===//
Constraint MulConstraintImpl(op: Op) [{
  return mlir::success(
      cast<tensorrt::ElementWiseOp>(op).getElementwiseOperation() ==
        ElementWiseOperation::kPROD
    );
}];
Constraint AddConstraintImpl(op: Op) [{
  return mlir::success(
      cast<tensorrt::ElementWiseOp>(op).getElementwiseOperation() ==
        ElementWiseOperation::kSUM
    );
}];
Constraint TanhConstraintImpl(op: Op) [{
  return mlir::success(
      cast<tensorrt::ActivationOp>(op).getActivationType() ==
        ActivationType::kTANH
    );
}];
Constraint MaxConstraintImpl(op: Op) [{
    return mlir::success(
        cast<tensorrt::ElementWiseOp>(op).getElementwiseOperation() ==
        ElementWiseOperation::kMAX
    );
}];
Constraint MinConstraintImpl(op: Op) [{
    return mlir::success(
        cast<tensorrt::ElementWiseOp>(op).getElementwiseOperation() ==
        ElementWiseOperation::kMIN
    );
}];
Constraint PowConstraintImpl(op: Op) [{
    return mlir::success(
        cast<tensorrt::ElementWiseOp>(op).getElementwiseOperation() ==
        ElementWiseOperation::kPOW
    );
}];
Constraint ErfConstraintImpl(op: Op) [{
  return mlir::success(
      cast<tensorrt::UnaryOp>(op).getUnaryOperation() ==
        UnaryOperation::kERF
    );
}];
Constraint MulConstraint(op: Op<tensorrt.element_wise>) -> Op {
  MulConstraintImpl(op);
  return op;
}
Constraint AddConstraint(op: Op<tensorrt.element_wise>) -> Op {
  AddConstraintImpl(op);
  return op;
}
Constraint TanhConstraint(op: Op<tensorrt.activation>) -> Op {
  TanhConstraintImpl(op);
  return op;
}
Constraint MaxConstraint(op: Op<tensorrt.element_wise>) -> Op {
  MaxConstraintImpl(op);
  return op;
}
Constraint MinConstraint(op: Op<tensorrt.element_wise>) -> Op {
  MinConstraintImpl(op);
  return op;
}
Constraint PowConstraint(op: Op<tensorrt.element_wise>) -> Op {
  PowConstraintImpl(op);
  return op;
}
Constraint ErfConstraint(op: Op<tensorrt.unary>) -> Op {
  ErfConstraintImpl(op);
  return op;
}
Constraint Mul(lhs: Value, rhs: Value) -> Op {
  return MulConstraint(op<tensorrt.element_wise>(lhs, rhs));
}
Constraint Add(lhs: Value, rhs: Value) -> Op {
  return AddConstraint(op<tensorrt.element_wise>(lhs, rhs));
}
Constraint Tanh(x: Value) -> Op {
  return TanhConstraint(op<tensorrt.activation>(x));
}
Constraint Max(lhs: Value, rhs: Value) -> Op {
  return MaxConstraint(op<tensorrt.element_wise>(lhs, rhs));
}
Constraint Min(lhs: Value, rhs: Value) -> Op {
  return MinConstraint(op<tensorrt.element_wise>(lhs, rhs));
}
Constraint Pow(lhs: Value, rhs: Value) -> Op {
  return PowConstraint(op<tensorrt.element_wise>(lhs, rhs));
}
Constraint Erf(x: Value) -> Op {
  return ErfConstraint(op<tensorrt.unary>(x));
}

Rewrite GetSplatElementAttr(x: Value) -> Attr [{
  return *getSplatConstantElementAttribute(x);
}];

Constraint HasSplatElements(x: Value) [{
  return LogicalResult(getSplatConstantElementAttribute(x));
}];

/// Is true if `x` is a constant op that has a splat constant
/// where splat element is equal to `attr`.
Constraint SplatElements(x: Value, attr: Attr) [{
  FailureOr<Attribute> value = getSplatConstantElementAttribute(x);
  if(LogicalResult(value).failed()) return failure();
  if(*value == attr) return success();
  FloatAttr fvalue = dyn_cast<FloatAttr>(*value);
  FloatAttr fattr = dyn_cast<FloatAttr>(attr);
  return success(fvalue && fattr && std::abs(fvalue.getValueAsDouble() - fattr.getValueAsDouble()) < .001); // handle different floating point type
}];

/// We need a native C++ function since we can't create the right
/// enum type in PDLL.
Rewrite CreateGeluTanh(x: Value) -> Op [{
  return rewriter.create<tensorrt::ActivationOp>(x.getLoc(),
    x, ActivationType::kGELU_TANH, FloatAttr{}, FloatAttr{}
  );
}];
Rewrite CreateGeluErf(x: Value) -> Op [{
  return rewriter.create<tensorrt::ActivationOp>(x.getLoc(),
    x, ActivationType::kGELU_ERF, FloatAttr{}, FloatAttr{}
  );
}];

Rewrite CreateClipActivation(x: Value, min: Attr, max: Attr) -> Op [{
  return rewriter.create<tensorrt::ActivationOp>(x.getLoc(),
    x, ActivationType::kCLIP, cast<FloatAttr>(max), cast<FloatAttr>(min));
}];

Constraint TypesMatch(x: Value, y: Value) [{
  return success(x.getType() == y.getType());
}];

/// Raise a sequence of "approximate" GELU to `tensorrt.ext.gelu_tanh`.
/// See
/// `https://github.com/google/jax/blob/main/jax/_src/nn/functions.py#L424-L455`.
Pattern RaiseToGeluTanh {
  let x: Value;
  let const0: Value;
  SplatElements(const0, attr<"4.471500e-02 : f32">);
  let rootPiOverTwo: Value;
  SplatElements(rootPiOverTwo, attr<"0.797884583 : f32">);
  let one: Value;
  SplatElements(one, attr<"1.0 : f32">);
  let half: Value;
  SplatElements(half, attr<"0.5 : f32">);
  let scaledCube = Mul(Mul(Mul(x, x), x), const0);
  let tanArg = Mul(Add(x, scaledCube), rootPiOverTwo);
  let inner = Add(Tanh(tanArg), one);
  let root = Mul(x, Mul(inner, half));

  // Sanity check for cases where we could have broadcasted x.
  TypesMatch(root, x);

  rewrite root with {
    let replacement = CreateGeluTanh(x);
    replace root with replacement;
  };
}

/// Raise a sequence of "approximate" GELU to `tensorrt.ext.gelu_tanh`.
/// Matching pattern of Ops from PyTorch/torch-mlir
Pattern RaiseToGeluTanh2 {
  let x: Value;
  let half2: Value;
  SplatElements(half2, attr<"0.5 : f32">);
  let one: Value;
  SplatElements(one, attr<"1.0 : f32">);
  let three: Value;
  SplatElements(three, attr<"3.0 : f32">);
  let const0: Value;
  SplatElements(const0, attr<"4.471500e-02 : f32">);
  let scaledCube = Mul(Pow(x, three), const0);
  let half1: Value;
  SplatElements(half1, attr<"0.5 : f32">);
  let twoOverPi: Value;
  SplatElements(twoOverPi, attr<"0.63661977236 : f32">);
  let sqrt2pi = Pow(twoOverPi, half1);
  let tanArg = Mul(sqrt2pi, Add(x, scaledCube));
  let inner = Add(Tanh(tanArg), one);
  let root = Mul(Mul(x, half2), inner);

  // Sanity check for cases where we could have broadcasted x.
  TypesMatch(root, x);

  rewrite root with {
    let replacement = CreateGeluTanh(x);
    replace root with replacement;
  };
}

/// Raise a sequence of GELU to `tensorrt.ext.gelu_none`.
/// Matching pattern of Ops from PyTorch/torch-mlir
Pattern RaiseToGeluErf {
  let x: Value;
  let half: Value;
  SplatElements(half, attr<"0.5 : f32">);
  let one: Value;
  SplatElements(one, attr<"1.0 : f32">);
  let const0: Value;
  SplatElements(const0, attr<"7.070310e-01 : f32">);
  let erf = Erf(Mul(x, const0));
  let normalCdf = Mul(Add(erf, one), half);
  let root = Mul(x, normalCdf);

  rewrite root with {
    let replacement = CreateGeluErf(x);
    replace root with replacement;
  };
}

/// Raise a elementwise min/max to the CLIP activation
/// Matching pattern of Ops from PyTorch/torch-mlir
Pattern RaiseMaxMinToClip {
  let x: Value;
  let minValue: Value;
  let maxValue: Value;
  let root = Min(minValue, Max(x, maxValue));

  TypesMatch(root, x);

  HasSplatElements(minValue);
  HasSplatElements(maxValue);

  rewrite root with {
    let min: Attr = GetSplatElementAttr(minValue);
    let max: Attr = GetSplatElementAttr(maxValue);
    let replacement = CreateClipActivation(x, min, max);
    replace root with replacement;
  };
}
