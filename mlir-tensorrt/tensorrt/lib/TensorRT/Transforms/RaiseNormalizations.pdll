//===- RaiseNormalizations.pdll --------------------------------*- PDLL -*-===//
//
// SPDX-FileCopyrightText: Copyright 2024 NVIDIA CORPORATION & AFFILIATES.
// All rights reserved.
// SPDX-License-Identifier: Apache-2.0
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
//===----------------------------------------------------------------------===//
///
/// PDLL rewrites for matching normalization patterns.
///
//===----------------------------------------------------------------------===//
#include "mlir-tensorrt-dialect/TensorRT/IR/TensorRTOps.td"
#include "mlir-tensorrt-dialect/TensorRT/IR/TensorRTEnums.td"

//===----------------------------------------------------------------------===//
// Native constraints
//===----------------------------------------------------------------------===//

/// Check all the binary and unary operations with their respective enums

Constraint MulConstraintImpl(op: Op) [{
  return mlir::success(
      cast<tensorrt::ElementWiseOp>(op).getElementwiseOperation() ==
        ElementWiseOperation::kPROD
    );
}];

Constraint AddConstraintImpl(op: Op) [{
  return mlir::success(
      cast<tensorrt::ElementWiseOp>(op).getElementwiseOperation() ==
        ElementWiseOperation::kSUM
    );
}];

Constraint SubConstraintImpl(op: Op) [{
  return mlir::success(
      cast<tensorrt::ElementWiseOp>(op).getElementwiseOperation() ==
        ElementWiseOperation::kSUB
    );
}];

Constraint DivConstraintImpl(op: Op) [{
  return mlir::success(
      cast<tensorrt::ElementWiseOp>(op).getElementwiseOperation() ==
        ElementWiseOperation::kDIV
    );
}];

Constraint ReciprocalConstraintImpl(op: Op) [{
  return mlir::success(
      cast<tensorrt::UnaryOp>(op).getUnaryOperation() ==
        UnaryOperation::kRECIP
    );
}];

Constraint SqrtConstraintImpl(op: Op) [{
  return mlir::success(
      cast<tensorrt::UnaryOp>(op).getUnaryOperation() ==
        UnaryOperation::kSQRT
    );
}];

/// Check constraint is a splat element and has the value
/// equal to the number of HW elements from input type
Constraint CheckNumHWElementsAndInputRank(x: Op,  inputType: Type) [{
  DenseElementsAttr els{};
  if(!matchPattern(x, m_Constant(&els)))
    return failure();
  if(!els.isSplat() || !els.getElementType().isF32())
    return failure();
  RankedTensorType rtt = cast<RankedTensorType>(inputType);
  if(rtt.getRank() < 2)
    return failure();
  auto hw = ArrayRef(rtt.getShape()).take_back(2);
  auto splatValue = els.getSplatValue<float>();
  float eps = 0.0001;
  return success(splatValue - hw[0]*hw[1]  < eps);
}];

/// Check constraint is a splat element and has the value
/// equal to the number of HW elements from input type
Constraint CheckEPS(val: Value) [{
  DenseElementsAttr els{};
  if(!matchPattern(val, m_Constant(&els)))
    return failure();
  if(!els.isSplat() || !els.getElementType().isF32())
    return failure();
  auto splatValue = els.getSplatValue<float>();
  float eps = 1e-5;
  return success(splatValue <= eps);
}];

/// Check that the last two dimensions of the input are flattened.
Constraint FlattenConstraintImpl(val: Value)[{
  ReshapeOp reshapeOp = cast<tensorrt::ReshapeOp>(val.getDefiningOp());
  RankedTensorType inputType = reshapeOp.getInput().getType();
  RankedTensorType outputType = reshapeOp.getResult().getType();
  int64_t inputRank = inputType.getRank();
  int64_t outputRank = outputType.getRank();
  if(inputRank < 2 || outputRank < 1)
    return failure();
  auto hw = ArrayRef(inputType.getShape()).take_back(2);
  int64_t numHWInput = hw[0] * hw[1];
  return success(
          inputRank == outputRank + 1
       && numHWInput == outputType.getDimSize(outputRank-1)
    );

}];

/// Check that the input is expanded by two ranks at tail.
Constraint ExpandTailDimsImpl(val: Value)[{
  auto expandDimsOp = cast<tensorrt::ExpandRankOp>(val.getDefiningOp());
  RankedTensorType inputType = expandDimsOp.getInput().getType();
  RankedTensorType outputType = expandDimsOp.getResult().getType();
  for(int i = 0; i < inputType.getRank(); ++i){
    if(inputType.getDimSize(i) != outputType.getDimSize(i))
      return failure();
  }
  for(int i = outputType.getRank()-1; i == inputType.getRank(); --i){
    if(outputType.getDimSize(i) != 1)
      return failure();
  }
  return success(inputType.getRank() + 2 == outputType.getRank());
}];

/// Check that the reduction operation is a sum and that the reduction
/// happens at the last dimension.
Constraint ReduceSumImpl(val: Value)[{
 ReduceOp reduceOp = cast<tensorrt::ReduceOp>(val.getDefiningOp());
 return success(reduceOp.getReduceOperation() == ReduceOperation::kSUM
    &&  reduceOp.getReduceAxes().size() == 1
    && (reduceOp.getReduceAxes()[0] ==
              (reduceOp.getInput().getType().getRank() - 1)));
}];

Constraint CheckRank4(val: Value)[{
 RankedTensorType rtt = cast<RankedTensorType>(val.getType());
 return success(rtt.getRank() == 4);
}];

//===----------------------------------------------------------------------===//
// Layernorm Op Constraints
//===----------------------------------------------------------------------===//

Constraint Add(lhs: Value, rhs: Value) {
    let addOp = op<tensorrt.element_wise>(lhs, rhs);
    AddConstraintImpl(addOp);
    return addOp;
}

Constraint Mul(lhs: Value, rhs: Value) -> Value{
    let mulOp = op<tensorrt.element_wise>(lhs, rhs);
    MulConstraintImpl(mulOp);
    return mulOp.0;
}

Constraint Div(lhs: Value, rhs: Value) -> Value{
    let divOp = op<tensorrt.element_wise>(lhs, rhs);
    DivConstraintImpl(divOp);
    return divOp;
}

Constraint Sub(lhs: Value, rhs: Value) -> Value{
    let subOp = op<tensorrt.element_wise>(lhs, rhs);
    SubConstraintImpl(subOp);
    return subOp;
}

Constraint Reciprocal(val: Value) -> Value{
    let recipOp =  op<tensorrt.unary>(val);
    ReciprocalConstraintImpl(recipOp);
    return recipOp;
}

Constraint SquareRoot(val: Value)-> Value{
    let sqrtOp = op<tensorrt.unary>(val);
    SqrtConstraintImpl(sqrtOp);
    return sqrtOp;
}

Constraint ReverseSqrt(val : Value) -> Value{
    return SquareRoot(Reciprocal(val));
}

Constraint FlattenTailDims(val: Value) -> Value {
 CheckRank4(val);
 let reshapeRes = op<tensorrt.reshape>(val);
 FlattenConstraintImpl(reshapeRes);
 return reshapeRes;
}

Constraint ReduceSum(val: Value) -> Value{
    let reduceOp = op<tensorrt.reduce>(val);
    ReduceSumImpl(reduceOp);
    return reduceOp;
}

Constraint ExpandTailDims(val: Value) -> Value{
    let expandVal = op<tensorrt.expand_rank>(val);
    CheckRank4(expandVal);
    ExpandTailDimsImpl(expandVal);
    return expandVal;
}

Constraint Mean(input: Value, numHW: Value){
 return Div(ExpandTailDims(ReduceSum(FlattenTailDims(input))), numHW);
}

Pattern RaiseInstanceNormalization_NCHW {
   let inputType : Type;
   let input : Value<inputType>;
   let scale : Value;
   let offset : Value;
   let numHW = op<tensorrt.constant>();
   CheckNumHWElementsAndInputRank(numHW, inputType);
   let eps = op<tensorrt.constant>();
   CheckEPS(eps);
   let mean = Mean(input, numHW);
   let xMinusMean = Sub(input, mean);
   let xMinusMeanSq = Mul(xMinusMean, xMinusMean);
   let stdDev = Mean(xMinusMeanSq, numHW);
   let stdDevPlusEps = Add(stdDev, eps);
   let rhs = ReverseSqrt(stdDevPlusEps);
   let mulWithScale = Mul(scale, rhs);
   let prod = Mul(mulWithScale, xMinusMean);
   let addOffset = Add(prod, offset);
   CheckRank4(addOffset);
  replace addOffset with op<tensorrt.normalization>(input, scale, offset){axis = attr<"array<i64: 2,3>">};
}
