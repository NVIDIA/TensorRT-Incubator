//===- StablehloToLinalg.cpp ----------------------------------------------===//
//
// Copyright (c) 2025, NVIDIA CORPORATION. All rights reserved.
//
//===----------------------------------------------------------------------===//
#include "mlir-tensorrt/Conversion/Passes.h"
#include "mlir/Dialect/Affine/IR/AffineOps.h"
#include "mlir/Dialect/Arith/IR/Arith.h"
#include "mlir/Dialect/Arith/Utils/Utils.h"
#include "mlir/Dialect/Bufferization/IR/Bufferization.h"
#include "mlir/Dialect/Complex/IR/Complex.h"
#include "mlir/Dialect/Linalg/IR/Linalg.h"
#include "mlir/Dialect/Linalg/Transforms/Transforms.h"
#include "mlir/Dialect/Math/IR/Math.h"
#include "mlir/Dialect/SCF/IR/SCF.h"
#include "mlir/Dialect/Shape/IR/Shape.h"
#include "mlir/Dialect/Tensor/IR/Tensor.h"
#include "mlir/IR/PatternMatch.h"
#include "mlir/Interfaces/FunctionInterfaces.h"
#include "mlir/Transforms/DialectConversion.h"
#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
#include "stablehlo/conversions/linalg/transforms/Rewriters.h"
#include "stablehlo/conversions/linalg/transforms/TypeConversion.h"
#include "stablehlo/dialect/StablehloOps.h"

namespace mlir {
#define GEN_PASS_DEF_STABLEHLOTOLINALGPASS
#include "mlir-tensorrt/Conversion/Passes.h.inc"
} // namespace mlir

using namespace mlir;

/// Strip algorithm attribute from dot_general when safe for Linalg conversion.
static LogicalResult stripDotGeneralAlgorithm(stablehlo::DotGeneralOp op) {
  auto algorithm = op.getAlgorithm();
  if (!algorithm)
    return failure();

  // Extract element types using modern cast API
  auto lhsElemType = op.getLhs().getType().getElementType();
  auto rhsElemType = op.getRhs().getType().getElementType();
  auto resultElemType = op.getType().getElementType();

  // Verify algorithm can be safely removed
  if (algorithm->getLhsPrecisionType() != lhsElemType ||
      algorithm->getRhsPrecisionType() != rhsElemType)
    return failure(); // Precision mismatch

  Type accType = algorithm->getAccumulationType();
  bool validAccumulator = false;

  // Check if accumulator matches result type
  if (accType == resultElemType)
    validAccumulator = true;
  else if (lhsElemType.isF16() && rhsElemType.isF16() && accType.isF32())
    validAccumulator = true;
  else if (lhsElemType.isBF16() && rhsElemType.isBF16() && accType.isF32())
    validAccumulator = true;
  else if (lhsElemType.isTF32() && rhsElemType.isTF32() && accType.isF32())
    validAccumulator = true;

  if (!validAccumulator)
    return failure();

  if (algorithm->getLhsComponentCount() != 1 ||
      algorithm->getRhsComponentCount() != 1 ||
      algorithm->getNumPrimitiveOperations() != 1)
    return failure();

  // Create new op without algorithm attribute
  OpBuilder builder(op);
  auto newOp = builder.create<stablehlo::DotGeneralOp>(
      op.getLoc(), op.getType(), op.getLhs(), op.getRhs(),
      op.getDotDimensionNumbers(), op.getPrecisionConfig().value_or(nullptr),
      /*algorithm=*/nullptr);

  op->replaceAllUsesWith(newOp);
  op.erase();

  return success();
}

/// Match indexing map '-DN + const' and return the dimension in 'dim'.
static bool matchReverseIndexingExpr(AffineExpr expr, unsigned &dim,
                                     int64_t &offset) {
  auto addExpr = dyn_cast<AffineBinaryOpExpr>(expr);
  if (!addExpr || addExpr.getKind() != mlir::AffineExprKind::Add)
    return false;

  auto mulExpr = dyn_cast<AffineBinaryOpExpr>(addExpr.getLHS());
  if (!mulExpr || mulExpr.getKind() != mlir::AffineExprKind::Mul)
    return false;

  // Match the negative one.
  auto constExpr = dyn_cast<AffineConstantExpr>(mulExpr.getRHS());
  if (!constExpr || constExpr.getValue() != -1)
    return false;

  // Match dimension mexpr
  auto dimExpr = dyn_cast<AffineDimExpr>(mulExpr.getLHS());
  if (!dimExpr)
    return false;

  // Match the constant of the add.
  auto constOffset = dyn_cast<AffineConstantExpr>(addExpr.getRHS());
  if (!constOffset)
    return false;

  dim = dimExpr.getPosition();
  offset = constOffset.getValue();
  return true;
}

/// Matching indexing map where each result is either a dimension iterator or a
/// reversal of an iterator.
static bool matchReverseIndexingMap(OpOperand *inputOperand, AffineMap map,
                                    ArrayRef<int64_t> staticLoopRanges) {
  if (map.getNumResults() != staticLoopRanges.size())
    return false;

  SmallVector<AffineExpr> newResults;
  bool hasReverse{false};
  for (auto [idx, expr] : llvm::enumerate(map.getResults())) {
    unsigned dim{0};
    int64_t offset{0};
    if (auto dimExpr = dyn_cast<AffineDimExpr>(expr))
      continue;

    if (!matchReverseIndexingExpr(expr, dim, offset))
      return false;
    if (staticLoopRanges[dim] != offset + 1)
      return false;
    hasReverse = true;
  }
  return hasReverse;
}

namespace {

/// Rewrite 'linalg.generic' that have an iteration map containing results like
/// '-d0 + dimSize-1'. These are currently generated by e.g. stablehlo.reverse,
/// but the upstream linalg transformations have poor support for correctly
/// handling the maps. Instead, we can drop the input and use `linalg.index` to
/// gather the required value. See:
/// https://github.com/llvm/llvm-project/issues/113021.
struct FixupReverseLinalgGenericPattern
    : public OpRewritePattern<linalg::GenericOp> {
  using OpRewritePattern::OpRewritePattern;

  LogicalResult matchAndRewrite(linalg::GenericOp op,
                                PatternRewriter &rewriter) const override {
    SmallVector<int64_t> staticLoopRanges = op.getStaticLoopRanges();
    SmallVector<std::tuple<Value, AffineMap, unsigned>> operandsToReplace;

    for (auto [idx, operand] : llvm::enumerate(op.getDpsInputOperands())) {
      if (op.isScalar(operand))
        continue;
      AffineMap indexingMap = op.getMatchingIndexingMap(operand);
      if (!matchReverseIndexingMap(operand, indexingMap, staticLoopRanges))
        continue;
      BlockArgument matchingArg = op.getRegionInputArgs()[idx];
      if (matchingArg.use_empty())
        continue;
      operandsToReplace.push_back({operand->get(), indexingMap, idx});
    }

    if (operandsToReplace.empty())
      return failure();

    rewriter.setInsertionPointToStart(op.getBlock());
    SmallVector<OpFoldResult> point;
    for (auto idx : llvm::seq<unsigned>(0, op.getNumLoops()))
      point.push_back(
          rewriter.create<linalg::IndexOp>(op.getLoc(), idx).getResult());

    for (auto [operand, indexingMap, idx] : operandsToReplace) {
      BlockArgument matchingArg = op.getRegionInputArgs()[idx];
      // Materialize the extract.
      SmallVector<Value> coords =
          llvm::map_to_vector(affine::makeComposedFoldedMultiResultAffineApply(
                                  rewriter, op.getLoc(), indexingMap, point),
                              [&](OpFoldResult result) {
                                return mlir::getValueOrCreateConstantIndexOp(
                                    rewriter, op.getLoc(), result);
                              });
      Value extracted =
          rewriter.create<tensor::ExtractOp>(op.getLoc(), operand, coords);
      rewriter.replaceAllUsesWith(matchingArg, extracted);
      rewriter.finalizeOpModification(op);
    }
    return success();
  }
};

class StablehloToLinalgPass
    : public impl::StablehloToLinalgPassBase<StablehloToLinalgPass> {
  using Base::Base;

  LogicalResult initialize(MLIRContext *context) override {
    target = std::make_shared<ConversionTarget>(*context);
    target->addLegalDialect<
        bufferization::BufferizationDialect, arith::ArithDialect,
        complex::ComplexDialect, linalg::LinalgDialect, math::MathDialect,
        tensor::TensorDialect, scf::SCFDialect, shape::ShapeDialect>();
    target->addLegalOp<UnrealizedConversionCastOp>();
    target->addDynamicallyLegalDialect<stablehlo::StablehloDialect>(
        [](Operation *op) -> std::optional<bool> {
          if (isa<stablehlo::ScatterOp>(op))
            return std::nullopt;

          if (isa<stablehlo::CustomCallOp>(op))
            return std::nullopt;

          // Check if parent is op like `stablehlo.reduce`.
          Operation *parent = op->getParentOp();
          while (parent) {
            if (isa<FunctionOpInterface>(parent))
              return false;
            if (isa_and_present<stablehlo::StablehloDialect>(
                    parent->getDialect()))
              return true;
            parent = parent->getParentOp();
          }
          return false;
        });

    patterns = [&] {
      RewritePatternSet patterns_(context);
      populateStablehloToLinalgConversionPatterns(context, converter,
                                                  &patterns_,
                                                  /*enablePrimitiveOps=*/false,
                                                  /*enableSparseOps=*/false);
      return patterns_;
    }();

    cleanupPatterns = [&] {
      RewritePatternSet cleanupPatterns_(context);
      cleanupPatterns_.add<FixupReverseLinalgGenericPattern>(context);
      linalg::populateEraseUnusedOperandsAndResultsPatterns(cleanupPatterns_);
      return cleanupPatterns_;
    }();

    return success();
  }

  void runOnOperation() override {
    // First, strip algorithm attributes from dot_general ops
    getOperation()->walk([&](stablehlo::DotGeneralOp dotOp) {
      // Attempt to strip the algorithm attribute; failures are silently ignored
      // as they indicate the op should keep its algorithm attribute
      (void)stripDotGeneralAlgorithm(dotOp);
    });

    // Then apply the conversion patterns
    if (failed(applyPartialConversion(getOperation(), *target, patterns))) {
      emitError(getOperation()->getLoc(), "failed to apply conversion in ")
          << getArgument();
      return signalPassFailure();
    }

    if (failed(applyPatternsGreedily(getOperation(), cleanupPatterns))) {
      emitError(getOperation()->getLoc(), "failed to apply conversion in ")
          << getArgument();
      return signalPassFailure();
    }
  }

private:
  std::shared_ptr<ConversionTarget> target;
  FrozenRewritePatternSet patterns;
  FrozenRewritePatternSet cleanupPatterns;
  stablehlo::LinalgTypeConverter converter;
};
} // namespace
